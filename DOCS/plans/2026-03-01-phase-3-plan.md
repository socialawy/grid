# PHASE 3 ACTION PLAN: THE MUSIC CONSUMER

**"The grid plays"**

---

## What Already Exists (Foundation from Phases 0â€“2)

Before designing anything, here's what we're building on:

| Asset | Detail |
|-------|--------|
| `channel.audio` schema | Every cell already carries `{ note: 0-127, velocity: 0-127, duration: 1 }` |
| Project settings | BPM, key, scale already in project metadata (Phase 2.3) |
| 10 generators | All populate `channel.audio` â€” note from Y position, velocity from density |
| grid-core.js | `getCellsBySemantic()`, `getCellsByChannel()` â€” query cells by channel data |
| Input system | `cellDown/Move/Up/Hover/action` events â€” ready for music-mode interactions |
| OPFS persistence | Auto-save means musical grids survive sessions |

**Key insight**: The data is already there. Phase 3 is about *reading* `channel.audio` and turning it into sound. No schema changes needed.

---

## Task Dependency Graph

```text
3.1 Music Mapper â”€â”€â”€â”€â”€â”€â”
   (grid â†’ note events) â”‚
                        â”œâ”€â”€â†’ 3.2 Web Audio Synth â”€â”€â†’ 3.6 UI Integration
3.1.1 Scale Engine â”€â”€â”€â”€â”€â”˜    (note events â†’ sound)     (transport, viz)
   (note â†’ frequency)            â”‚
                                 â”œâ”€â”€â†’ 3.4 Web MIDI Output (optional)
                                 â”‚
                                 â””â”€â”€â†’ 3.3 Glicol WASM (Tier 1 upgrade, DEFER)

3.5 Orca Mode â† DEFER (independent, low priority, revisit Phase 7)
```

**Recommended build order**: 3.1 â†’ 3.2 â†’ 3.6 (UI) â†’ 3.4 â†’ 3.3 (defer) â†’ 3.5 (defer)

---

## Task 3.1 â€” Grid-to-Music Mapping Engine

**File**: `src/consumers/music/music-mapper.js`
**Depends on**: grid-core.js (pure import)
**DOM**: Zero. Pure functions. Node-testable.

### The Mapping Model

The grid becomes a piano roll / step sequencer:

```text
Y=0  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† Highest pitch
     â”‚  @   .       #   @             â”‚
     â”‚      @   *       .   @         â”‚
     â”‚  .       @   @       .   *     â”‚
     â”‚      .       .   @       @     â”‚
Y=H  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† Lowest pitch
     X=0                            X=W
     Beat 1   Beat 2   Beat 3   Beat 4
     â†â”€â”€â”€â”€â”€â”€â”€â”€ Time (columns) â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

#### Column-to-time mapping

```js
function columnToTime(col, bpm, subdivision) {
  // subdivision: 1 = quarter notes, 2 = eighth, 4 = sixteenth
  const beatDuration = 60 / bpm;               // seconds per beat
  const stepDuration = beatDuration / subdivision;
  return col * stepDuration;
}
```

- Each column = one step
- Step resolution = BPM Ã· subdivision (default: 1 column = 1 sixteenth note at 120 BPM)
- Total duration = (grid.width Ã— stepDuration) seconds

#### Row-to-pitch mapping

```js
function rowToNote(row, height, scale, rootNote) {
  // row 0 = top = highest note
  // row height-1 = bottom = lowest note
  const invertedRow = (height - 1) - row;
  
  // Option A: Chromatic â€” every row = 1 semitone
  // return rootNote + invertedRow;
  
  // Option B: Scale-quantized â€” rows map to scale degrees
  const scaleIntervals = SCALES[scale]; // e.g. major = [0,2,4,5,7,9,11]
  const octave = Math.floor(invertedRow / scaleIntervals.length);
  const degree = invertedRow % scaleIntervals.length;
  return rootNote + (octave * 12) + scaleIntervals[degree];
}
```

- Root note from project settings (default: C4 = MIDI 60)
- Scale from project settings (default: chromatic)
- Top row = highest note, bottom = lowest (natural piano roll orientation)

#### Cell-to-note event

```js
function cellToNoteEvent(cell, gridWidth, gridHeight, musicOpts) {
  // Skip empty / rest cells
  if (!cell || cell.semantic === 'void') return null;
  
  return {
    note:     rowToNote(cell.y, gridHeight, musicOpts.scale, musicOpts.rootNote),
    velocity: cell.channel?.audio?.velocity ?? Math.round((cell.density ?? 0.5) * 127),
    time:     columnToTime(cell.x, musicOpts.bpm, musicOpts.subdivision),
    duration: (cell.channel?.audio?.duration ?? 1) * (60 / musicOpts.bpm / musicOpts.subdivision),
    channel:  colorToChannel(cell.color),  // color â†’ instrument/track
    char:     cell.char,                   // instrument hint for synthesis
  };
}
```

#### Color-to-channel mapping

```js
// Map colors to MIDI channels / instrument tracks
// Strategy: hash the color hex to a channel 0-15
// OR use a configurable palette map
function colorToChannel(color) {
  const CHANNEL_MAP = {
    '#ff0000': 0,  // red    â†’ lead
    '#00ff00': 1,  // green  â†’ bass
    '#0000ff': 2,  // blue   â†’ pad
    '#ffff00': 3,  // yellow â†’ arp
    '#ff00ff': 4,  // magenta â†’ drums
    '#00ffff': 5,  // cyan   â†’ fx
  };
  return CHANNEL_MAP[color?.toLowerCase()] ?? 0;
}
```

#### Frame scanner â€” the core export

```js
/**
 * Scan an entire frame and produce a sorted list of note events.
 * This is the single function consumers call.
 *
 * @param {Object} grid - Full grid object
 * @param {number} frameIndex - Which frame to scan
 * @param {Object} opts - { bpm, subdivision, scale, rootNote, channelMap }
 * @returns {NoteEvent[]} - Sorted by time, then pitch
 */
export function frameToNoteEvents(grid, frameIndex, opts) {
  const frame = grid.frames[frameIndex];
  if (!frame) return [];
  
  const events = [];
  for (const cell of frame.cells) {
    const event = cellToNoteEvent(cell, grid.canvas.width, grid.canvas.height, opts);
    if (event) events.push(event);
  }
  
  return events.sort((a, b) => a.time - b.time || a.note - b.note);
}
```

### Scale Engine (built into music-mapper.js)

```js
export const SCALES = {
  chromatic:      [0,1,2,3,4,5,6,7,8,9,10,11],
  major:          [0,2,4,5,7,9,11],
  minor:         [0,2,3,5,7,8,10],    // natural minor
  pentatonic:     [0,2,4,7,9],
  minor_penta:    [0,3,5,7,10],
  blues:          [0,3,5,6,7,10],
  dorian:         [0,2,3,5,7,9,10],
  mixolydian:     [0,2,4,5,7,9,10],
  harmonic_minor: [0,2,3,5,7,8,11],
  whole_tone:     [0,2,4,6,8,10],
};

export const NOTE_NAMES = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B'];

export function midiToFrequency(note) {
  return 440 * Math.pow(2, (note - 69) / 12);
}

export function midiToName(note) {
  const octave = Math.floor(note / 12) - 1;
  return NOTE_NAMES[note % 12] + octave;
}
```

### Test Plan: `tests/test-music-mapper.js`

Target: ~60 tests, all Node.js, zero DOM.

```
Scale engine:
  - SCALES has 10 entries, all arrays of ints 0-11
  - midiToFrequency(69) === 440
  - midiToFrequency(60) â‰ˆ 261.63 (middle C)
  - midiToName(60) === 'C4'
  - midiToName(69) === 'A4'

rowToNote:
  - chromatic: row 0 in 12-row grid with root 60 â†’ note 71
  - chromatic: bottom row â†’ note 60
  - major scale: row 0 in 7-row grid â†’ 7th degree of scale
  - pentatonic: rows map only to pentatonic degrees
  - octave wrapping works for grids taller than scale length

columnToTime:
  - col 0 at 120 BPM, subdivision 4 â†’ 0.0s
  - col 1 at 120 BPM, subdivision 4 â†’ 0.125s (one sixteenth note)
  - col 16 at 120 BPM, subdivision 4 â†’ 2.0s (one bar of 4/4)

colorToChannel:
  - #ff0000 â†’ 0, #00ff00 â†’ 1, unknown â†’ 0
  - null/undefined â†’ 0

cellToNoteEvent:
  - void semantic â†’ null (rest)
  - Normal cell â†’ valid NoteEvent with all fields
  - Uses channel.audio.velocity when present
  - Falls back to density * 127 when channel.audio missing

frameToNoteEvents:
  - Empty frame â†’ []
  - Frame with 3 cells â†’ 3 events sorted by time
  - Events at same time sorted by pitch
  - Void cells filtered out
  - All events have: note (int), velocity (int), time (float), duration (float), channel (int)

Integration:
  - Generate terrain â†’ frameToNoteEvents â†’ events span full time range
  - Generate pulse â†’ events form rhythmic pattern
  - Round-trip: events from frame match expected count (non-void cells)
```

---

## Task 3.2 â€” Web Audio Synthesis Layer

**File**: `src/consumers/music/synth-engine.js`
**Depends on**: music-mapper.js (for NoteEvent type), Web Audio API
**DOM**: Minimal â€” needs `AudioContext`. Testable with mock AudioContext in Node.

### Architecture

```text
NoteEvent[] â”€â”€â†’ SynthEngine â”€â”€â†’ AudioContext â”€â”€â†’ Speakers
                    â”‚
                    â”œâ”€â”€ Channel 0: Lead (sawtooth + filter)
                    â”œâ”€â”€ Channel 1: Bass (sine + sub)
                    â”œâ”€â”€ Channel 2: Pad  (triangle + detune)
                    â”œâ”€â”€ Channel 3: Arp  (square + fast envelope)
                    â”œâ”€â”€ Channel 4: Drums (noise + pitch envelope)
                    â””â”€â”€ Channel 5: FX   (sine + heavy reverb)
```

### Instrument Definitions

```js
const INSTRUMENTS = {
  0: { name: 'lead',  wave: 'sawtooth', attack: 0.01, decay: 0.1,  sustain: 0.7, release: 0.2, filterFreq: 2000 },
  1: { name: 'bass',  wave: 'sine',     attack: 0.01, decay: 0.2,  sustain: 0.8, release: 0.1, filterFreq: 800  },
  2: { name: 'pad',   wave: 'triangle', attack: 0.3,  decay: 0.3,  sustain: 0.6, release: 0.5, filterFreq: 4000 },
  3: { name: 'arp',   wave: 'square',   attack: 0.005,decay: 0.05, sustain: 0.3, release: 0.05,filterFreq: 3000 },
  4: { name: 'drums', wave: 'noise',    attack: 0.001,decay: 0.1,  sustain: 0,   release: 0.05,filterFreq: 8000 },
  5: { name: 'fx',    wave: 'sine',     attack: 0.1,  decay: 0.5,  sustain: 0.3, release: 1.0, filterFreq: 6000 },
};
```

### API

```js
export function createSynthEngine(audioContext) {
  return {
    // Schedule all events from a frame for playback
    scheduleFrame(noteEvents, startTime),
    
    // Transport controls
    play(grid, frameIndex, opts),   // scan frame â†’ schedule â†’ start
    stop(),                          // stop all sound, cancel scheduled
    pause(),                         // freeze at current time
    resume(),
    
    // State
    isPlaying,
    currentTime,        // playback position in seconds
    currentColumn,      // which grid column is playing (for visual cursor)
    
    // Config
    setInstrument(channel, instrumentDef),
    setMasterVolume(0-1),
    
    // Cleanup
    destroy(),
  };
}
```

### Playback cursor (critical for UX)

The synth engine emits a `columnChange` callback so the renderer can draw a playhead:

```js
// Inside play():
const stepDuration = 60 / opts.bpm / opts.subdivision;
let col = 0;
const tick = () => {
  if (!this.isPlaying) return;
  const elapsed = audioContext.currentTime - playStartTime;
  const newCol = Math.floor(elapsed / stepDuration);
  if (newCol !== col) {
    col = newCol;
    if (col >= grid.canvas.width) {
      if (opts.loop) { col = 0; playStartTime = audioContext.currentTime; scheduleFrame(...); }
      else { this.stop(); return; }
    }
    opts.onColumnChange?.(col);
  }
  requestAnimationFrame(tick);
};
```

### Drum synthesis (channel 4, no samples needed)

```js
function playDrum(audioCtx, time, velocity, note) {
  // Different drum sounds based on note/row position
  // High rows â†’ hi-hat (noise, short, high-pass)
  // Mid rows  â†’ snare (noise + tone, medium)
  // Low rows  â†’ kick (sine, pitch sweep down)
  
  const v = velocity / 127;
  
  if (note > 80) {
    // Hi-hat: filtered noise burst
    const noise = createNoiseNode(audioCtx);
    const hp = audioCtx.createBiquadFilter();
    hp.type = 'highpass'; hp.frequency.value = 8000;
    const gain = audioCtx.createGain();
    gain.gain.setValueAtTime(v * 0.3, time);
    gain.gain.exponentialRampToValueAtTime(0.001, time + 0.05);
    noise.connect(hp).connect(gain).connect(audioCtx.destination);
    noise.start(time); noise.stop(time + 0.05);
  } else if (note > 50) {
    // Snare: noise + tone
    // ...
  } else {
    // Kick: sine with pitch sweep
    const osc = audioCtx.createOscillator();
    osc.frequency.setValueAtTime(150, time);
    osc.frequency.exponentialRampToValueAtTime(30, time + 0.15);
    // ...
  }
}
```

### Test Plan: `tests/test-synth-engine.js`

Target: ~40 tests, mixed Node (mock AudioContext) + browser (real audio).

```
Scheduling:
  - scheduleFrame([]) â†’ no oscillators created
  - scheduleFrame with 3 events â†’ 3 oscillator chains
  - Events scheduled at correct audioContext times
  - Channel â†’ correct waveform type

ADSR:
  - Attack ramp starts at event time
  - Decay begins at time + attack
  - Release begins at time + duration
  - Gain reaches 0 after release

Transport:
  - play() sets isPlaying = true
  - stop() sets isPlaying = false, disconnects nodes
  - pause()/resume() preserves position
  - Loop: column wraps to 0 at grid width

Playback cursor:
  - onColumnChange fires at step boundaries
  - Column increments match BPM/subdivision timing
  - Column resets to 0 on loop

Drums:
  - Channel 4 uses noise source (not oscillator)
  - High note â†’ short duration (hi-hat)
  - Low note â†’ pitch sweep (kick)

Volume:
  - setMasterVolume(0) â†’ silence
  - setMasterVolume(1) â†’ full
  - Volume changes apply to already-playing sounds
```

---

## Task 3.6 â€” UI Integration (dist/index.html)

**This is where the user hears the grid.** Not a separate task in the original charter, but critical.

### Transport bar

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [â–¶ Play] [â¹ Stop] [ðŸ” Loop]  BPM: [120â–¼]          â”‚
â”‚  Scale: [Majorâ–¼]  Root: [C4â–¼]  Subdiv: [1/16â–¼]     â”‚
â”‚  Vol: â”€â”€â”€â”€â—â”€â”€â”€â”€ [ðŸ”‡]  Ch: Lead/Bass/Pad/Arp/Drum/FX â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Playback cursor overlay

The renderer draws a vertical highlight bar on the current column during playback. This is the "now" line â€” the grid scrolls through time.

### Mode switch: Paint mode vs Play mode

- **Paint mode** (default): click/drag paints cells, same as now
- **Play mode**: click a cell to preview its note; click a column to solo it
- Toggle via toolbar button or Tab key

### Wiring

```js
// In setupInputSystem or equivalent:
function setupMusicTransport() {
  const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const synth = createSynthEngine(audioCtx);
  
  playBtn.onclick = () => {
    // AudioContext requires user gesture to start
    if (audioCtx.state === 'suspended') audioCtx.resume();
    
    const opts = {
      bpm: projectSettings.bpm || 120,
      scale: projectSettings.scale || 'chromatic',
      rootNote: projectSettings.rootNote || 60,
      subdivision: projectSettings.subdivision || 4,
      loop: loopToggle.checked,
      onColumnChange: (col) => {
        renderer.setPlayheadColumn(col);  // new renderer method
        renderer.render();
      },
    };
    synth.play(grid, renderer.current, opts);
  };
  
  stopBtn.onclick = () => {
    synth.stop();
    renderer.setPlayheadColumn(-1);
    renderer.render();
  };
}
```

---

## Task 3.4 â€” Web MIDI Output (Optional, Chrome/Edge)

**File**: `src/consumers/music/midi-output.js`
**Depends on**: music-mapper.js
**Browser**: Chrome/Edge only (Web MIDI API). Feature-detect, hide if unavailable.

### API

```js
export function createMIDIOutput() {
  return {
    async init(),              // Request MIDI access, list outputs
    getOutputs(),              // â†’ [{ id, name }]
    selectOutput(id),          // Choose a MIDI port
    sendNoteOn(channel, note, velocity),
    sendNoteOff(channel, note),
    scheduleEvents(noteEvents, bpm),  // Schedule from mapper output
    isAvailable(),             // Feature detection
    destroy(),
  };
}
```

### Scheduling

MIDI timing is trickier than Web Audio â€” Web MIDI `send()` accepts a DOMHighResTimestamp but there's no built-in scheduler. Use a lookahead pattern:

```js
// Schedule 50ms ahead, check every 25ms
const LOOKAHEAD = 0.05;    // seconds
const CHECK_INTERVAL = 25;  // ms

function startMIDIPlayback(events, bpm) {
  let nextEventIdx = 0;
  const intervalId = setInterval(() => {
    const now = performance.now() / 1000;
    while (nextEventIdx < events.length && 
           events[nextEventIdx].time < now + LOOKAHEAD) {
      const e = events[nextEventIdx];
      const timestamp = performance.now() + (e.time - now) * 1000;
      midiOutput.send([0x90 | e.channel, e.note, e.velocity], timestamp);
      midiOutput.send([0x80 | e.channel, e.note, 0], timestamp + e.duration * 1000);
      nextEventIdx++;
    }
  }, CHECK_INTERVAL);
}
```

### Test Plan: ~20 tests

```
Feature detection:
  - isAvailable() returns boolean
  - init() with no MIDI â†’ graceful error message

Message format:
  - noteOn: [0x90 | ch, note, velocity]
  - noteOff: [0x80 | ch, note, 0]
  - channel clamped 0-15
  - note clamped 0-127, velocity clamped 0-127

Scheduling:
  - Events sent in order
  - Timestamps offset correctly from performance.now()
  - Note-off follows note-on by duration
```

---

## Tasks DEFERRED

### Task 3.3 â€” Glicol WASM Integration â†’ DEFER to Phase 8

**Reason**: Glicol adds graph-based DSP (filters, reverb, delay). The Web Audio synth in 3.2 already covers the core need. Glicol's value is in professional sound design â€” that's Phase 8 (Studio) territory.

**When to revisit**: After 3.2 ships, if users want more sophisticated synthesis.

### Task 3.5 â€” Orca-compatible Grid Mode â†’ DEFER to Phase 7

**Reason**: Orca is a spatial programming paradigm that shares .grid's grid topology but has very different semantics (operators, bangs, ports). Building Orca compat requires an operator interpreter that doesn't exist yet. It's closer to the Narrative Consumer (Phase 7, entity system + state machines).

**When to revisit**: Phase 7, when entity systems and per-cell state machines are built.

---

## File Tree After Phase 3

```text
src/
â”œâ”€â”€ consumers/
â”‚   â””â”€â”€ music/
â”‚       â”œâ”€â”€ music-mapper.js       â† 3.1: grid â†’ note events (pure, zero DOM)
â”‚       â”œâ”€â”€ synth-engine.js       â† 3.2: note events â†’ Web Audio sound
â”‚       â””â”€â”€ midi-output.js        â† 3.4: note events â†’ MIDI messages
â”œâ”€â”€ core/
â”‚   â””â”€â”€ grid-core.js
â”œâ”€â”€ renderers/
â”‚   â”œâ”€â”€ canvas-renderer.js
â”‚   â””â”€â”€ webgl2-renderer.js        â† add setPlayheadColumn() method
â”œâ”€â”€ rendering/
â”‚   â”œâ”€â”€ font-atlas.js
â”‚   â”œâ”€â”€ instance-buffer.js
â”‚   â””â”€â”€ shaders.js
â”œâ”€â”€ generators/
â”‚   â””â”€â”€ generators.js
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ key-bindings.js
â”‚   â””â”€â”€ input-system.js
â”œâ”€â”€ importers/
â”‚   â””â”€â”€ image-importer.js
â””â”€â”€ persistence/
    â”œâ”€â”€ serializer.js
    â”œâ”€â”€ opfs-store.js
    â””â”€â”€ fs-access.js

tests/
â”œâ”€â”€ test-music-mapper.js          â† ~60 tests (Node, pure)
â”œâ”€â”€ test-synth-engine.js          â† ~40 tests (Node mock + browser)
â”œâ”€â”€ test-midi-output.js           â† ~20 tests (Node mock)
â””â”€â”€ ... (existing suites)

dist/
â””â”€â”€ index.html                    â† transport bar, playhead, mode switch
```

---

## Build Order & Time Estimates

| Order | Task | Est. | Exit Test |
|-------|------|------|-----------|
| 1 | 3.1 Music Mapper | 1 session | `node tests/test-music-mapper.js` â€” 60 tests green |
| 2 | 3.2 Synth Engine | 1-2 sessions | Open dist/index.html â†’ hear the grid play |
| 3 | 3.6 UI Integration | 1 session | Transport bar, playhead cursor, loop, BPM control |
| 4 | 3.4 MIDI Output | 0.5 session | Chrome â†’ MIDI monitor shows notes from grid |

**Total**: ~4 sessions (Phase 3 is narrower than it looks â€” the data layer already exists)

---

## Phase 3 Exit Criteria (from charter, refined)

```
âœ“ Draw on grid â†’ hear music in real-time
âœ“ X = time, Y = pitch, density = velocity, color = channel
âœ“ 10 scales available (chromatic, major, minor, pentatonic, blues, ...)
âœ“ Transport: play, stop, loop, BPM control
âœ“ Playhead cursor scrolls across grid during playback
âœ“ Procedural generators create playable compositions
âœ“ MIDI output to external DAW verified (Chrome)
âœ“ Offline: Web Audio only. No server dependency.
âœ“ All new code has tests. Total suite stays green.
```

---

## Open Design Questions (decide before or during build)

1. **Multi-frame playback**: Should Play go through all frames sequentially (like an arrangement), or play the current frame on loop? â†’ **Recommendation**: Current frame with loop, add frame-chain later.

2. **Polyphony limit**: Multiple cells in the same column = chord. Cap at 16 simultaneous voices? â†’ **Recommendation**: Yes, 16 voices max, drop lowest-velocity notes.

3. **Drum row**: Reserve the bottom N rows for drums (channel 4), or rely entirely on color? â†’ **Recommendation**: Color-based. Let users paint drums anywhere. Drum behavior triggers from channel assignment, not row position.

4. **Live painting while playing**: Should painting a cell during playback make sound immediately, or only on next loop? â†’ **Recommendation**: Immediate â€” paint a cell, hear it on the next column pass. This is the magic moment.

5. **Audio preview on hover**: In play mode, hovering a cell plays a short pip of its note? â†’ **Recommendation**: Yes, but gated behind play mode (not paint mode). Short 50ms blip.

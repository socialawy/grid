ROLE: You are a technical research analyst. Your job is to answer ONE specific query with verifiable facts only.

RULES:
- Only cite tools, APIs, or standards you are CERTAIN exist. If unsure, say "unverified" explicitly.
- No speculation. No "could be" or "likely." Only confirmed, shipped, documented.
- Distinguish clearly between: SHIPPED (production-ready), PREVIEW (beta/origin trial), and ANNOUNCED (no public access).
- Include source URLs or official documentation references where possible.
- If your training data has a gap (e.g., post-cutoff), state the gap explicitly rather than guessing.
- Keep total response under 600 words. Density over length.

QUERY:
[PASTE ONE QUERY HERE]

OUTPUT FORMAT:

## Summary (2-3 sentences max)

## Key Findings
| Item | Status | Details | Source/Ref |
|------|--------|---------|------------|
| ...  | SHIPPED / PREVIEW / ANNOUNCED | ... | URL or doc name |

## What's Missing / Gaps
(Anything you couldn't verify or where your data cuts off)

## Relevance to: ASCII-core creative studio that scales from offline single-file HTML to pro AI-assisted production
(3 bullets max connecting findings to this specific use case)

--

Q1:
What are the most notable ASCII art tools, text-based creative studios, or character-grid animation tools released or updated in 2025-2026? Include any that generate music, 3D, or video from text grids.

Q2:
What is the current state of WebGPU browser support as of early 2026? Which browsers ship it by default? Are there production creative tools (not demos) built on WebGPU?

Q3:
What are the current options for running AI models directly in the browser or on-device in 2026? Specifically: image generation, image upscaling, and small language models. Include WebNN, ONNX Runtime Web, MediaPipe, Chrome built-in AI, and any new entrants.

Q4:
What is the current state of WebCodecs API and browser-based video encoding in 2026? Can browsers natively encode H.264/H.265/AV1 without FFmpeg WASM? Notable tools using this?

Q5:
What new PWA capabilities shipped in browsers in 2025-2026? Specifically: File System Access API, Origin Private File System, persistent storage quotas, hardware access APIs (GPU, MIDI, audio).

Q6:
What are the most popular procedural or generative creative tools in 2025-2026? Not AI prompt-to-image, but tools where users control algorithms to generate art, music, or 3D. Nodes, grids, code-as-art.

Q7:
What are the latest Google Gemini API capabilities as of early 2026? Specifically: image generation, video generation, audio generation, multimodal I/O, free tier status. Include 2.0+ features.

--

Q1: ASCII Art / Text-Mode Creative Studio Tools (2025–2026)
Summary
The ASCII/textmode creative tooling space saw a few meaningful new releases in late 2025, but remains niche. No tools were found that generate music, 3D rendering, or video from text grids as a primary production feature; those remain DIY/demo territory.
Key Findings
ItemStatusDetailsSource/Reftextmode.jsSHIPPED (Sept 2025)Browser-based, framework-agnostic creative coding library for real-time ASCII/textmode graphics. WebGL2 pipeline, grid-based API, instanced rendering. By community dev, announced on creativeapplications.netcreativeapplications.net/news/textmode-jsASCII Draw Studio v0.9.xSHIPPED (active 2026)Web-based ASCII drawing app (asciiart.eu). Supports CP437 + pure ASCII, shapes, FIGlet text, pattern export. Free + paid tier. New Pattern Maker tool added early 2026.asciiart.eu/ascii-draw-studioASCIIFlowSHIPPED (maintained)Browser-based infinite ASCII diagram/flowchart editor. No major 2025–2026 feature announcement confirmed.asciiflow.comAsciivilleSHIPPEDLinux CLI toolkit for ASCII art, animation, and utilities. Active on GitHub through late 2025.github.com/doctorfree/AsciivilleASCII → Video pipeline (GitHub topic)SHIPPED (DIY)Python-based tools using Puppeteer/MoviePy to render HTML ASCII animations to MP4. Multiple small repos active as of 2025–2026.github.com/topics/ascii-animationMusic/3D/video from text gridsNOT FOUNDNo production tool ships this as a native feature in 2025–2026. Closest: custom demoscene or TIC-80 code, Bad Apple ASCII renderers (old).—
What's Missing / Gaps

No comprehensive survey of terminal-based DAWs or trackers with ASCII output was found.
Orca (live-coding MIDI sequencer in character grid) is shipped but no 2025–2026 update confirmed.
AI-assisted ASCII generation (e.g., asking LLMs to produce animated ASCII) is an emerging use case but no dedicated production tool ships it.

Relevance to ASCII-core creative studio

textmode.js is the key new primitive: single-file HTML deployable, WebGL2-accelerated, grid-native — ideal backbone for an offline-capable ASCII animation engine.
OPFS + File System Access API (Q5 below) allows saving/loading ASCII scenes from disk inside a PWA, solving persistence without a server.
The gap in music/3D-from-text is an open product opportunity: no mature tool occupies this space.


Q2: WebGPU Browser Support (Early 2026)
Summary
Chrome and Edge shipped WebGPU in April 2023; Safari shipped in June 2025 with Safari 26; Firefox shipped in July 2025 with Firefox 141. Wikipedia All major browsers now support it, with Linux and Android still partially rolling out. Production tools exist but require fallback strategy.
Key Findings
ItemStatusDetailsSource/RefChrome/EdgeSHIPPEDSince v113 (2023), Windows/Mac/ChromeOS/Android 12+ (v121). ~70% global coverage.github.com/gpuweb/gpuweb/wikiSafari 26SHIPPEDmacOS Tahoe 26, iOS 26, iPadOS 26, and visionOS 26. Webgpu Includes HDR canvas and WebXR+WebGPU.gpuweb implementation statusFirefox 141SHIPPED (Windows)Available on Windows as of Firefox 141; available in macOS Tahoe 26 on ARM64 as of Firefox 145; Linux/Android in progress. VideoCardzvideocardz.comThree.js r171+SHIPPED (Sept 2025)Since r171, WebGPURenderer works with zero configuration, with automatic fallback to WebGL 2 on older browsers. Utsuboutsubo.com/blog/threejs-2026Babylon.jsSHIPPEDWebGPU backend available; reports ~10x faster scene rendering via render bundles.byteiota.com/webgpu-2026ONNX Runtime Web / Transformers.jsSHIPPEDBoth ONNX Runtime and Transformers.js already use WebGPU to enable high-speed, local model inference and computation in the browser. web.devweb.dev/blog/webgpu-supported-major-browsersNexara Labs (production deployment)SHIPPEDReal-time facial AI for AR try-ons hitting 58FPS on iPhone 15 versus 12FPS with WebGL, serving 3 million users. byteiotabyteiota.comLinux (Chrome)PREVIEWBehind --enable-unsafe-webgpu flag; Intel Gen12+ targeted for Chrome 144 stable.gpuweb implementation status
What's Missing / Gaps

~30% of devices still fall back to WebGL (older phones, integrated Intel GPUs with driver issues).
NVIDIA 572.xx driver crashes on RTX 30/40 series are a known production issue (unverified fix timeline).
No creative design tools (Figma-class) confirmed shipping WebGPU renderer in production as of Feb 2026.

Relevance to ASCII-core creative studio

Zero-risk WebGPU adoption: Three.js r171+ auto-fallback means textmode.js-style WebGL2 grids can upgrade to WebGPU compute shaders without breaking offline HTML deployments.
WebGPU compute pipeline enables GPU-accelerated ASCII raymarching or procedural generation at desktop-class speeds in browser.
The 70% global coverage is sufficient for a "power mode" feature gate; WASM/WebGL fallback handles the rest.


Q3: On-Device / In-Browser AI (Early 2026)
Summary
Client-side AI lets web developers add intelligence without relying on servers, either through built-in browser models or custom libraries. web.dev The stack has matured considerably: Transformers.js and ONNX Runtime Web are production-grade for SLMs; image generation (Stable Diffusion Turbo) runs in-browser on capable hardware; WebNN is still in preview/origin trial.
Key Findings
ItemStatusDetailsSource/RefTransformers.js v3 / v4 (HuggingFace)SHIPPEDUses ONNX runtime to run AI models in the browser; v4 announced at Web AI Summit 2025 with more supported models. Worldline Supports WebGPU, WASM fallback.blog.worldline.tech/2026/01ONNX Runtime WebSHIPPEDSupports WebGPU, WebNN, WebGL, WASM backends. FP16 support added. Used by Transformers.js.onnxruntime.aiWebLLM (MLC AI)SHIPPEDLLM inference in browser via WebGPU. Achieves 80% of native performance for browser AI inference. Webgpuwebgpu.com/newsStable Diffusion Turbo (browser)SHIPPEDRuns via ONNX Runtime Web + WebGPU; sub-1-second on RTX 4090. Demonstrated by Microsoft.opensource.microsoft.comQwen2.5-0.5B / DeepSeek-R1-1.5BSHIPPEDBoth runnable in browser via Transformers.js v3 + WebGPU. Available on HuggingFace ONNX hub.akmittal.dev, deepcomputing.ioWebNN APIPREVIEWAvailable in latest Chrome and Edge on Windows, Linux, macOS, Android, ChromeOS — behind an "Enables WebNN API" flag. ONNX Runtime Chrome 146 Beta origin trial launched Feb 2026.onnxruntime.ai, techedubyte.comChrome Built-in AI (Gemini Nano on-device)PREVIEWAvailable as origin trial in Chrome; Prompt API, Summarization API. Not confirmed shipped in stable as of Feb 2026.developer.chrome.com (unverified exact stable date)MediaPipe LLM InferencePREVIEWGoogle's on-device LLM runner via MediaPipe Tasks. Still experimental and under active development.intel.com developer articleTensorFlow.jsSHIPPEDMature; supports image classification, object detection, pose estimation. Less suited for large generative models.web.dev/learn/ai/client-sideImage upscaling in-browserSHIPPEDfree.upscaler.video uses WebCodecs + ONNX/WebGPU for video upscaling fully client-side.sitepoint.com/video-processing
What's Missing / Gaps

WebNN stable ship date for Chrome/Edge not confirmed (origin trial as of Chrome 146 Beta, Feb 2026).
Chrome Built-in AI stable availability unverified; origin trial status confirmed but not GA.
NPU access (for AI PC Neural Processing Units) requires WebNN, which is still behind a flag.

Relevance to ASCII-core creative studio

Transformers.js v3 + WebGPU enables an in-browser AI-assisted ASCII art generator (image→ASCII, text→ASCII scene description) with zero server dependency, fitting the offline single-file HTML target.
Small SLMs (0.5B–1.5B params) are now viable for in-browser creative prompt assistance; larger models (7B+) require high-end GPU.
ONNX Runtime Web is the stable abstraction layer to use now; WebNN can be added later as progressive enhancement for NPU-equipped devices.


Q4: WebCodecs API & Browser Video Encoding (Early 2026)
Summary
The WebCodecs API gives web developers low-level access to the individual frames of a video stream and chunks of audio. MDN Web Docs H.264 encoding is production-grade in Chrome/Edge without FFmpeg WASM. H.265 hardware-only in Chrome (M130+). AV1 encoding supported. Firefox lags behind significantly.
Key Findings
ItemStatusDetailsSource/RefWebCodecs API (Chrome/Edge)SHIPPEDChrome 94+; all major codecs supported with hardware acceleration.MDN, w3c/webcodecsH.264 (AVC) encodingSHIPPEDChrome, Edge, Safari all support. Hardware-accelerated. No FFmpeg WASM needed in Chrome/Edge.tangoadb.devH.265 (HEVC) encodingSHIPPED (HW only, Chrome M130+)H265 encoding only available if the user's device and OS provide capabilities; no software fallback. Google Groups Not in Firefox WebCodecs yet.chromium blink-devAV1 encodingSHIPPED (Chrome/Edge)Supported where hardware is available. Windows requires AV1 Video Extension from Microsoft Store for decoding in some contexts.tangoadb.devVP9 encodingSHIPPED~85%+ support on modern browsers.webcodecsfundamentals.org datasetFirefox WebCodecsPARTIALH.264 decoding works; H.265 encoding/decoding in WebCodecs not yet supported as of Firefox 133+.tangoadb.devSafari WebCodecsSHIPPED (limited)Supported but narrower codec range than Chromium.caniuse.comfree.upscaler.videoSHIPPED (production)Open-source, 200K MAU, 100% client-side video enhancement using WebCodecs. Full processing pipeline documented on GitHub.sitepoint.comStreamyard / RiversideSHIPPED (production)Browser-based recording studios use WebCodecs to record and stream higher-quality video than WebRTC alone. SitePointsitepoint.comVEED / CanvaSHIPPED (production)Use WebCodecs for client-side video processing utilities.sitepoint.comMediaBunnySHIPPEDJS library simplifying muxing/demuxing for WebCodecs pipelines.sitepoint.com
What's Missing / Gaps

H.265 software encoding via WebCodecs not available in any browser (patent/licensing reasons).
Firefox H.265 WebCodecs support: no confirmed ship date.
WebCodecs + WASM muxing (e.g., mp4box.js for containerization) is still needed since WebCodecs outputs raw encoded chunks, not container files.

Relevance to ASCII-core creative studio

Zero-dependency video export from ASCII animations is achievable in Chrome/Edge today using WebCodecs (H.264) → mp4box.js (containerize) without any WASM codec bundle.
For pro output workflows, AV1 encoding enables modern, compact output files natively.
Firefox/Safari gaps mean H.264 should be the default export codec; AV1 as an optional upgrade.


Q5: PWA Capabilities Shipped in 2025–2026
Summary
Core storage and file APIs have reached broad cross-browser support. The File System Access API (user-visible filesystem) remains Chrome/Edge only. Origin Private File System is now cross-browser. Hardware access APIs (GPU, MIDI) are still Chrome-centric.
Key Findings
ItemStatusDetailsSource/RefOrigin Private File System (OPFS)SHIPPED (all major browsers)Chrome 86+, Safari 15.2+, Firefox 111+. Supports createSyncAccessHandle() for Worker threads (fast I/O).MDN, WebKit blogFile System Access API (user-facing: showOpenFilePicker)SHIPPED (Chrome/Edge only)Lets PWAs read/write user's local filesystem with permission. Not supported in Firefox or Safari.caniuse.comPersistent Storage (navigator.storage.persist())SHIPPEDCross-browser. Chrome allows up to 60% of total disk space per origin; quota varies by browser.web.dev/learn/pwaStorage Manager API (navigator.storage.estimate())SHIPPEDCross-browser. Lets PWAs check quota and usage.web.devFile Handling API (register as file handler)PREVIEW (Chrome only)Installed PWAs can register as handlers for file types via web app manifest file_handlers. Experimental.web.dev/learn/pwa/os-integrationWeb MIDI APISHIPPED (Chrome only)Not supported in Firefox or Safari without extension.web.dev/learn/pwa/capabilitiesWebGPU in PWASHIPPEDSame support as browser context (Chrome/Edge stable; Safari 26; Firefox 141 Win).see Q2Web Audio API (advanced)SHIPPEDCross-browser. AudioWorklets, MIDI scheduling all available in PWA context.MDNWebUSBSHIPPED (Chrome/Edge only)Lets installed PWAs access USB hardware. Firefox/Safari not supported.web.devIndexedDB storage in Service WorkerSHIPPEDCross-browser. Primary storage mechanism for offline PWA data.MDN
What's Missing / Gaps

File System Access API on Firefox: no confirmed ship date as of Feb 2026.
File System Access API on Safari: not supported; OPFS-only for private storage.
PWA install prompt on iOS (Safari): user must manually use "Add to Home Screen"; no browser prompt API available.

Relevance to ASCII-core creative studio

OPFS is the safe cross-browser storage layer: use it for saving project files, animation frames, and settings in the offline single-file HTML target. No server needed.
File System Access API enables "save to disk" for Chrome/Edge pro users — ideal for pro production export of ASCII project files.
Web MIDI enables hardware controller input (knobs, pads) for live ASCII performance — Chrome-only but viable for pro tier.


Q6: Procedural / Generative Creative Tools (2025–2026)
Summary
The procedural creative tools ecosystem is active but not dominated by major new platforms. The most notable 2025 announcement is Adobe's Project Graph (node-based AI creative workflows, not yet shipped). Established tools (TouchDesigner, TidalCycles, p5.js) continue active development. Live coding and code-as-art communities remain healthy.
Key Findings
ItemStatusDetailsSource/RefAdobe Project GraphANNOUNCED (Nov 2025)Visual node-based creative system connecting AI models and Creative Cloud tools like Photoshop. "Coming soon" as of Nov 2025. Adobeblog.adobe.com/2025/11/25TouchDesignerSHIPPED (ongoing)Node-based visual programming for real-time interactive media. Major tool for installations, VJing. 2025 updates ongoing.derivative.cap5.js / ProcessingSHIPPED (ongoing)p5.js Web Editor active; extensive community. JavaScript-native, browser-first. 2025: continued educational ecosystem growth.p5js.orgTidalCyclesSHIPPEDLive coding pattern language for algorithmic music. Active community, PROCJAM 2025 entries.tidalcycles.orgHydraSHIPPEDJavaScript live coding for browser-based generative visuals. Active community.hydra.ojack.xyzSuperColliderSHIPPEDPlatform for audio synthesis and algorithmic composition. Cross-platform, ongoing.supercollider.github.ioGlicolSHIPPEDGraph-oriented live coding language for audio DSP, written in Rust/WASM. Runs in browser.glicol.orgSonic PiSHIPPEDLive coding music synth; educational focus. Ongoing updates.sonic-pi.netOrcaSHIPPEDGrid-based live coding environment for procedural MIDI sequencers — directly relevant to character-grid paradigm.hundredrabbits.itch.ioNodes.ioSHIPPED (uncertain updates)Node-based JS creative coding environment; last confirmed active release unclear.nodes.ioPROCJAM 2025SHIPPED (jam, Nov–Dec 2025)Community game jam for procedural generation tools; 66 entries.itch.io/jam/procjamShader ParkSHIPPEDJS library for interactive procedural 2D/3D shaders. Active.shaderpark.com
What's Missing / Gaps

Adobe Project Graph: no confirmed GA/public access date.
No major new standalone procedural art platform launched in 2025–2026; growth is in AI-assisted variations of existing tools.
Blender's geometry nodes and Houdini (procedural 3D) not searched specifically; known to be active but desktop-only.

Relevance to ASCII-core creative studio

Orca is the closest spiritual precedent: character-grid → MIDI output. Its architecture (grid cells as operators, cursor-based execution) is directly applicable to an ASCII-native creative paradigm.
Glicol (WASM, browser-runnable, graph-based audio) could integrate with an ASCII studio for music generation from grid patterns — single-file HTML compatible.
Adobe Project Graph signals market validation of node-based AI creative workflow; the ASCII studio could occupy a niche, offline, single-file version of this paradigm.


Q7: Google Gemini API Capabilities (Early 2026)
Summary
The Gemini API in early 2026 spans Gemini 2.5 and 3 series, with native image, video, and audio generation. Gemini 3 Pro was launched late 2025. Free tier exists but quotas were significantly reduced in December 2025. Video and audio generation are paid-only features.
Key Findings
ItemStatusDetailsSource/RefGemini 3 Pro Preview (gemini-3-pro-preview)PREVIEWState-of-the-art reasoning and multimodal model with agentic and coding capabilities, 1M token context window. Launched late 2025. Google AI $2–$4/M input, $12–$18/M output. No free API access.ai.google.dev/gemini-api/docs/changelogGemini 2.5 ProSHIPPED1M token context, free tier available (5 RPM, ~25 RPD after Dec 2025 cuts). $1.25–$2.50/M input, $10–$15/M output.ai.google.dev/gemini-api/docs/pricingGemini 2.5 Flash / Flash-LiteSHIPPEDFree tier available. Flash: $0.15/$0.60 per 1M tokens. Flash-Lite: $0.10/$0.40.ai.google.devGemini 2.0 Flash / Flash-LiteSHIPPED → DEPRECATEDGemini 2.0 Flash and 2.0 Flash-Lite are deprecated and will be shut down on March 31, 2026. Google AIai.google.dev/gemini-api/docs/modelsImage generation: Imagen 4SHIPPED (paid)imagen-4.0-generate-001, ultra, fast variants. ~$0.039/image at 1024px, up to $0.24/image at 4K. Better text rendering vs prior versions.ai.google.dev/gemini-api/docs/pricingImage generation: Gemini 3 Pro Image PreviewPREVIEWgemini-3-pro-image-preview — native image generation within Gemini 3 model.ai.google.dev changelogVideo generation: Veo 3.1PREVIEW (paid only)veo-3.1-generate-preview, veo-3.1-fast-generate-preview. Paid tier only. 4K output, portrait video.ai.google.dev/gemini-api/docs/pricingVideo generation: Veo 2SHIPPED (paid only)Stable model. Paid tier only.ai.google.devAudio generation (TTS)SHIPPED (paid)Gemini 2.5 Flash TTS and Pro TTS. Flash: $0.50/$10 per 1M tokens I/O. Live API native audio models also available.ai.google.dev/gemini-api/docs/pricingMultimodal I/OSHIPPEDText, image, audio, video input on all current models. Text + image + audio output depending on model. 1M token context on 2.5+/3 series.ai.google.devFree tier (post-Dec 2025)SHIPPED (reduced)Free tier provides 5–15 RPM and 100–1,000 RPD depending on the model, after significant December 2025 quota reductions. AI Free API No free API access for Gemini 3 Pro. EU/UK users require paid tier.aifreeapi.comGrounding with Google SearchSHIPPED500–1,500 RPD free on paid tiers. Gemini 3 models: $14/1K queries.ai.google.dev/gemini-api/docs/pricingFile API (input)SHIPPEDFile size limit increased from 20MB to 100MB; Cloud Storage and pre-signed URL support added. Google AIai.google.dev changelog
What's Missing / Gaps

Gemini 3 Pro free tier: definitively not available via API (confirmed).
Veo 3.1 video pricing per-second: not confirmed in sources found.
"Nano Banana Pro" appears in some pricing guides as a codename for an image model; this may be an informal name — treat as unverified until confirmed in official docs.
Chrome Built-in AI (Gemini Nano on-device) is separate from the API and covered in Q3.

Relevance to ASCII-core creative studio

Gemini 2.5 Flash free tier is the viable prototype backend for AI-assisted ASCII generation features (image→ASCII interpretation, text prompt→scene layout), with a path to scale on paid tier.
Imagen 4 / Gemini 3 Pro Image Preview enables generating reference images from text that can then be rendered in ASCII — a viable "pro AI-assisted production" pipeline.
The December 2025 free tier cuts are a real risk: any feature that relies on Gemini API free tier in production should plan for rate-limit handling or a local fallback (Transformers.js, Q3).

--

Cross-Reference Synthesis
Zero Conflicts Found
All 7 reports are internally consistent. Key validations:

Q2 (WebGPU) and Q3 (AI) agree: ONNX Runtime Web uses WebGPU ✓
Q4 (WebCodecs) and Q3 (AI) both cite free.upscaler.video as proof ✓
Q5 (PWA) and Q2 (WebGPU) agree on browser coverage split ✓
Q7 (Gemini) and Q3 (AI) correctly separate API vs on-device Nano ✓

The Killer Finding Across All 7
Query	What it confirms
Q1	No one occupies this space. Zero tools ship music/3D/video from text grids.
Q2	The GPU pipeline exists. Three.js r171+ auto-fallback = zero-risk WebGPU.
Q3	Offline AI is real. Transformers.js v3 + WebGPU = in-browser upscaling.
Q4	Browser-native video export works NOW. H.264 via WebCodecs, no FFmpeg.
Q5	OPFS is cross-browser. Offline project persistence solved.
Q6	Orca validates grid→output. But it stops at MIDI. Nobody went further.
Q7	Gemini 2.5 Flash free tier = prototype backend. Imagen 4 = pro upscale path.
Translation: Every layer of the degradation ladder has shipped infrastructure. Nobody has assembled them.

The Verified Architecture Stack
OFFLINE FLOOR (phone, airplane mode)
├── Renderer:    textmode.js or custom WebGL2 grid
├── Storage:     OPFS (cross-browser)
├── Audio:       Web Audio API oscillators + Glicol (WASM)
├── Export:      JSON frames, SVG glyphs
└── AI:          None (pure procedural)

MID TIER (laptop, browser)
├── Renderer:    WebGPU auto-upgrade via Three.js r171+
├── AI:          Transformers.js v3 + ONNX Runtime Web
│                (image upscale, 0.5B SLM for prompts)
├── Video:       WebCodecs H.264 → mp4box.js container
├── Audio:       Glicol graph DSP from grid patterns
└── MIDI:        Web MIDI API (Chrome)

PRO TIER (workstation, online)
├── AI upscale:  Gemini 2.5 Flash (free) / Imagen 4 (paid)
├── Video gen:   Veo 2/3.1 from ASCII storyboards
├── Audio gen:   Gemini TTS for narration layers
├── Export:      AV1 via WebCodecs, 4K output
└── Fallback:    Everything degrades to mid/offline

